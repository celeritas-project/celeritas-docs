%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% File  : conference-papers/chep-2021/main.tex
%
% CHEP Conference paper
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font
\usepackage{hyperref}
\usepackage{color}
\usepackage{xspace}

\usepackage[
    shortcuts,
    acronym,
    nonumberlist,
    nogroupskip,
    nopostdot]{glossaries}

\usepackage[
    detect-none,
    binary-units]{siunitx}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setacronymstyle{long-short}
\makeglossaries

\newacronym{mc}{MC}{Monte Carlo}
\newacronym{lhc}{LHC}{Large Hadron Collider}
\newacronym{hpc}{HPC}{high performance computing}
\newacronym{hep}{HEP}{high energy physics}
\newacronym{olcf}{OLCF}{Oak Ridge Leadership Computing Facility}
\newacronym{ecp}{ECP}{Exascale Computing Project}
\newacronym{ornl}{ORNL}{Oak Ridge National Laboratory}
\newacronym{em}{EM}{electromagnetic}
\newacronym{fom}{FOM}{Figure of Merit}
%%---------------------------------------------------------------------------%%
% Hyperref setup
\definecolor{CiteColor}{rgb}{0, 0, 0.55}
\definecolor{LinkColor}{rgb}{0.2, 0.2, 0.2}
\definecolor{URLColor}{rgb}{0.62745098, 0.1254902 , 0.94117647}
\hypersetup{
  linkcolor=LinkColor,
  citecolor=CiteColor,
  urlcolor=URLColor,
  colorlinks=true
}

% SI units
\sisetup{range-phrase = \text{--},
  group-separator={,},
  per-mode=symbol,
  group-minimum-digits=3,
  range-units=single}

% Commands
\newcommand{\Cpp}{C\texttt{++}\xspace}

%%---------------------------------------------------------------------------%%
\begin{document}

%%---------------------------------------------------------------------------%%
\title{Celeritas: Toward GPU-based particle transport for detector simulations
    in LHC experiments\footnote{This manuscript has been authored by
    UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the US Department of
    Energy. The United States Government retains and the publisher, by accepting
    the article for publication, acknowledges that the United States Government
    retains a nonexclusive, paid-up, irrevocable, worldwide license to publish
    or reproduce the published form of this manuscript, or allow others to do
    so, for United States Government purposes. DOE will provide access to these
    results of federally sponsored research in accordance with the DOE Public
    Access Plan (http://energy.gov/downloads/doe-public-access-plan).}}
    %%
    \author{\firstname{Seth}
    \lastname{Johnson}\inst{1}\fnsep\thanks{\email{johnsonsr@ornl.gov}}
    %%
    \and
    \firstname{Philippe} \lastname{Canal}\inst{2}
    %%
    \and
    \firstname{Thomas} \lastname{Evans}\inst{1}
    %%
    \and
    \firstname{Soon Yung} \lastname{Jun}\inst{2}
    %%
    \and
    \firstname{Guilherme} \lastname{Lima}\inst{2}
    %%
    \and
    \firstname{Amanda} \lastname{Lund}\inst{3}
    %%
    \and
    \firstname{Vincent} \lastname{Pascuzzi}\inst{4}
    %%
    \and
    \firstname{Stefano} \lastname{Tognini}\inst{1}
}

\institute{Oak Ridge National Laboratory
    \and
    Fermi National Accelerator Laboratory
    \and
    Argonne National Laboratory
    \and
    Lawrence Berkeley National Laboratory
}

\abstract{
    Celeritas is awesome.
}

\maketitle

%%---------------------------------------------------------------------------%%
\section{Introduction}
\label{sec:introduction}

Upgrades to the \ac{lhc} and its detectors (including CMS, ALICE, and ATLAS)
demand a commensurate increase in modeling and simulation capacity that is out
of reach of traditional software but can be alleviated through the use of
advanced \ac{hpc} hardware that use GPUs for improved performance at low power
consumption.  Our objective is to provide the needed modeling capacity using a
new application \emph{Celeritas} that performs fast and accurate \ac{mc}
particle transport simulations on GPUs. Celeritas is designed to complement, not
replace, Geant4 and ultimately satisfy the detector response requirements as
defined in Ref.~\cite{the_hep_software_foundation_roadmap_2019} using  the
advanced architectures that will form the backbone of \ac{hpc} over the next
decade.

%% Need the basic project plan here

In this paper we will focus primarily on the components of the Celeritas
software architecture (Sec.~\ref{sec:code-arch}) that will enable high
performance on GPU hardware.  Particular attention is paid to the construction
of the Celeritas data model (Sec.~\ref{sec:data-model}), as memory management on
heterogeneous devices is critical to code performance, particularly for random
access algorithms such as \ac{mc}.  Improvements to the VecGeom geometry package to enable \ac{mc} transport on GPUs are discussed in Sec.~\ref{sec:vecgeom}.  Finally, we show some preliminary performance results on a mini-application (mini-app) in Sec.~\ref{sec:miniapp}.

%%---------------------------------------------------------------------------%%
\section{Code Architecture}
\label{sec:code-arch}

%%-----------------%%
\subsection{Overview}
\label{sec:overview}

In the short term, Celeritas is designed as a standalone application
that transport particles exclusively on device. To support robust and
rapid unit testing, its components are designed to run natively in \Cpp
on traditional CPUs regardless of whether CUDA is available for
on-device execution.

Like other GPU-enabled \ac{mc} transport codes such as
Shift~\cite{pandya_implementation_2016,hamilton_continuous-energy_2019}, the
low-level component code used by transport kernels is designed so that each
particle track corresponds to a single thread, since particle tracks once
created are independent of each other. There is therefore essentially no
cooperation between individual threads, facilitating the dual host/device
annotation of most of Celeritas. The allocation of secondary particles and the
initialization of new tracks from these secondaries both require CUDA-specific
programming, but those components are encapsulated so that both host and device
code can safely construct secondaries.

To support parallelizing our initial development over several team
members, and to facilitate refactoring and performance testing of code,
Celeritas uses a highly modular programming approach based on
composition rather than inheritance. As much as possible, each major
code component is built of numerous smaller components and interfaces
with as few other components as possible. The interfaces with other
components are furthermore

%%-------------------%%
\subsection{Data Model}
\label{sec:data-model}

%%-----------------------%%
\subsection{Physics Models}
\label{sec:physics-models}

This section describes the current physics models implemented on GPU. The
implementation has focused on sampling secondaries rather than calculating
on-the-fly atomic cross sections, because for the majority of models those data
can be precalculated in Geant4 and exported as tabular data.

%%------------------------------------%%
\subsubsection{Klein--Nishina}

% Seth
Had to figure out how to allocate storage for secondaries.

%%------------------------------------%%
\subsubsection{Bethe--Heitler}

% Vince

%%------------------------------------%%
\subsubsection{Positron annihilation}

$\textrm{e}^+ \to (\gamma, \gamma)$
% Soon

%%------------------------------------%%
\subsubsection{Moller/Bhabha scattering}

% Stefano
In Geant4, both Moller and Bhabha scatterings are sampled by a single
class method, which defines the criteria based on the incident particle,
performs the interaction (Moller for electrons and Bhabha for positrons), and
calculates the final incident and secondary particle states. Celeritas
follows a similar structure, where a MollerBhabha interactor class is
responsible for performing the interaction and calculating the final particle
states. However, in Celeritas, the sampling algorithms for each scattering
process are broken down into two separate classes, each of which responsible
solely for executing the sampling loop and returning the fraction of the energy
transferred during the scattering process to the main MollerBhabha interactor,
which then proceeds to calculate the final particle states. This higher
compartmentalization improves code testing and maintainability, and provides
an opportunity to test code performance in the future by splitting the current
mechanism into two different models and templating the interactor by particle
type.

%%------------------------------------%%
\subsubsection{Livermore photoelectric}

% Amanda

%%---------------------------------------------------------------------------%%
\section{VecGeom Geometry Package}
\label{sec:vecgeom}

%%---------------------------------------------------------------------------%%
\section{Mini-App Results}
\label{sec:miniapp}

%%---------------------------------------------------------------------------%%
\section{Acknowledgements}

Work for this paper was supported by Oak Ridge National Laboratory (ORNL), which is managed and operated by UT-Battelle, LLC, for the U.S. Department of Energy (DOE) under Contract No. DEAC05-00OR22725.
%%
This research was supported by the Exascale Computing
Project (ECP), project number 17-SC-20-SC. The ECP is a collaborative effort of
two DOE organizations, the Office of Science and the National Nuclear Security
Administration, that are responsible for the planning and preparation of a
capable exascale ecosystem---including software, applications, hardware,
advanced system engineering, and early testbed platforms---to support the
nation's exascale computing imperative.

%%---------------------------------------------------------------------------%%
\bibliography{references}
%%---------------------------------------------------------------------------%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of conference-papers/chep-2021/main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% File  : conference-papers/chep-2021/main.tex
%
% CHEP Conference paper
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font
\usepackage{hyperref}
\usepackage{color}
\usepackage{xspace}
\usepackage{booktabs}

\usepackage[
    shortcuts,
    acronym,
    nonumberlist,
    nogroupskip,
    nopostdot]{glossaries}

\usepackage[
    detect-none,
    binary-units]{siunitx}

% >>> from pandoc
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\},fontsize=\small}
% Add '' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
% <<< from pandoc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setacronymstyle{long-short}
\makeglossaries

\newacronym{mc}{MC}{Monte Carlo}
\newacronym{lhc}{LHC}{Large Hadron Collider}
\newacronym{hpc}{HPC}{high performance computing}
\newacronym{hep}{HEP}{high energy physics}
\newacronym{olcf}{OLCF}{Oak Ridge Leadership Computing Facility}
\newacronym{ecp}{ECP}{Exascale Computing Project}
\newacronym{ornl}{ORNL}{Oak Ridge National Laboratory}
\newacronym{em}{EM}{electromagnetic}
\newacronym{fom}{FOM}{Figure of Merit}
\newacronym{sm}{SM}{streaming multiprocessor}

%%---------------------------------------------------------------------------%%
% Hyperref setup
\definecolor{CiteColor}{rgb}{0, 0, 0.55}
\definecolor{LinkColor}{rgb}{0.2, 0.2, 0.2}
\definecolor{URLColor}{rgb}{0.62745098, 0.1254902 , 0.94117647}
\hypersetup{
  linkcolor=LinkColor,
  citecolor=CiteColor,
  urlcolor=URLColor,
  colorlinks=true
}

% SI units
\sisetup{range-phrase = \text{--},
  group-separator={,},
  per-mode=symbol,
  group-minimum-digits=3,
  range-units=single}

% Commands
\newcommand{\Cpp}{C\texttt{++}\xspace}

%%---------------------------------------------------------------------------%%
\begin{document}

%%---------------------------------------------------------------------------%%
% TODO: new title? novel features and performance analysis for GPU EM
% particle transport in the Celeritas code?
\title{Novel features and GPU performance analysis for EM particle transport in
  the Celeritas code%
  %
  \footnote{%
    This manuscript has been authored by
    UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the US Department of
    Energy. The United States Government retains and the publisher, by accepting
    the article for publication, acknowledges that the United States Government
    retains a nonexclusive, paid-up, irrevocable, worldwide license to publish
    or reproduce the published form of this manuscript, or allow others to do
    so, for United States Government purposes. DOE will provide access to these
    results of federally sponsored research in accordance with the DOE Public
    Access Plan (http://energy.gov/downloads/doe-public-access-plan).%
  }%
}%
%%
\author{%
  \firstname{Seth R.} \lastname{Johnson}\inst{1}%
  \fnsep\thanks{\email{johnsonsr@ornl.gov}}
  %%
  \and
  \firstname{Philippe} \lastname{Canal}\inst{2}
  %%
  \and
  \firstname{Thomas} \lastname{Evans}\inst{1}
  %%
  \and
  \firstname{Soon Yung} \lastname{Jun}\inst{2}
  %%
  \and
  \firstname{Guilherme} \lastname{Lima}\inst{2}
  %%
  \and
  \firstname{Amanda} \lastname{Lund}\inst{3}
  %%
  \and
  \firstname{Vincent R.} \lastname{Pascuzzi}\inst{4}
  %%
  \and
  \firstname{Stefano} \lastname{Tognini}\inst{1}
}%
%%
\institute{%
  Oak Ridge National Laboratory
  \and
  Fermi National Accelerator Laboratory
  \and
  Argonne National Laboratory
  \and
  Lawrence Berkeley National Laboratory
}%
%%
\abstract{%
  Celeritas is awesome.
}%
%%
\maketitle

%%---------------------------------------------------------------------------%%
\section{Introduction}
\label{sec:introduction}

The new High Luminosity Large Hadron Collider (HL-LHC) Era of the LHC, along with the 
upgrades in the detectors of its main experiments (CMS, ATLAS, ALICE, and LHCb), will 
result in a steep rise in computing resource usage, far beyond the expected 
availability within current funding scenarios \cite{the_hep_software_foundation_roadmap_2019}.
\ac{mc} simulations are a large component of that expected increase, whose demand can be 
alleviated through the use of advanced \ac{hpc} hardware that use GPUs for improved performance 
at low power consumption. Our objective is to provide the needed modeling capacity
using a new application, \emph{Celeritas}, that performs fast and accurate \ac{mc}
particle transport simulations on GPUs. Celeritas is designed to complement, not
replace, Geant4 and ultimately satisfy the detector response requirements as
defined in Ref.~\cite{the_hep_software_foundation_roadmap_2019} using the
advanced architectures that will form the backbone of \ac{hpc} over the next
decade.

%% Need the basic project plan here

Celeritas' short term goal entails the implementation of \ac{em} physics for photons and
charged leptons (see Sec.~\ref{TBD}) on geometry models currently used by the \ac{hep}
community. More specifically, on the CMS geometry. This choice provides us the opportunity 
to simulate \ac{em} showers, which are the most computationally intensive part of an
event, as a proof-of-principle. In order to achieve such compatibility with current geometry 
model files, Celeritas uses the VecGeom \cite{VecGeom:web} package. This is a GPU 
compatible library that provides navigation tools aimed towards optimizing calculations 
performed in the complex, multi-level, hierarchical geometries of \ac{hep} detectors \cite{Wenzel:2020zyn},
being both successfully integrated with Geant4 and tested within the CMS simulation software.
% NOTE: Should we add that validation of the navigation algs are underway?


%%%%%%%%%%%%%


In this scenario, this paper focuses primarily on the components of the Celeritas
software architecture (Sec.~\ref{code-architecture}) that will enable high
performance on GPU hardware.  Particular attention is paid to the construction
of the Celeritas data model (Sec.~\ref{data-model}), as memory management on
heterogeneous devices is critical to code performance, particularly for random
access algorithms such as \ac{mc}.  Improvements to the VecGeom geometry 
package to enable \ac{mc} transport on GPUs are discussed in Sec.~\ref{sec:vecgeom}. 
Finally, we show some preliminary performance results on a mini-application (mini-app)
in Sec.~\ref{sec:miniapp}.

%%---------------------------------------------------------------------------%%
\input{code-arch.tex}

%%---------------------------------------------------------------------------%%
\section{VecGeom Geometry Package}
\label{sec:vecgeom}

VecGeom is a geometry modeller library, written in \Cpp\ and based on the SIMD
concept, it was designed for use in particle transport
simulation \cite{VecGeom:web}. VecGeom has been successfully integrated with Geant4
since version 10.6. Tested within the official CMS simulation software, VecGeom
has shown performance improvements between 7\% to 13\% \cite{Pedro:2019mkq}.

VecGeom also provides navigation tools which optimize the geometry calculations
in the complex, multi-level, hierarchical geometry of realistic HEP
detectors \cite{Wenzel:2020zyn}. Eventually, its parallel navigation algorithms
are expected to be also integrated into the Geant4 toolkit.

Most of VecGeom functionality also compiles and runs on GPU devices, hence it is
being integrated into the Celeritas application. Realistic geometries like
CMS2018 can be parsed from GDML format and transferred into GPU memory.
Validation of the navigation algorithms on the GPU is currently underway.


%%---------------------------------------------------------------------------%%
\section{Mini-App Results}
\label{sec:miniapp}

Using the Celeritas components, we constructed a demonstration app to verify a
simple test problem against Geant4 results. It is the simplest physical
simulation we can run, with photon-only transport a single
Compton scattering process using the Klein--Nishina model.  It has a single
infinite material (aluminum) and a 100 MeV monodirectional point source.

The stepping kernel is parallel over particle tracks, with one launch per step,
and ``dead'' tracks ignored. Interaction lengths are sampling with
uniform-in-log-energy cross section calculations with linear interpolation. The
particle states include position and directions that are updated with each step.
Secondaries are allocated and constructed as part of each interaction, but they
are immediately killed and their energy deposited locally. Each energy
deposition event, whether from an absorbed electron or a cutoff photon,
allocates (using a \texttt{StackAllocator<Hit>} instance) a detector hit and
writes the deposited energy, position, direction, time, and track ID to global
memory.

An additional kernel processes the allocated vector of detector hits into
uniformly spaced detector bins. A final kernel performs a reduction on the
``alive'' state of particles to determine whether the simulation should
terminate.

The same code components were used to build GPU and CPU versions of the same
stepping process, although the CPU version steps through one track at a time
rather than many tracks in parallel.  Figure~\ref{fig:baseline} gives the
baseline performance of the two versions. The host code uses a single core of an
Intel "Cascade Lake" Xeon processor running at 2.3 GHz, compiled with GCC 8.3
and \texttt{-O3 -march=skylake-avx512 -mtune=skylake-avx512}. The device code
uses a single Nvidia Tesla V100 running at 1.53 GHz, compiled with CUDA 10.1 and
\texttt{-O3 --use\_fast\_math}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{fig/cpu-gpu-comparison}
  \caption{Performance comparison of the CPU and GPU versions of the Celeritas
  code. The ``total'' GPU plot includes the extra kernel launches for processing
  detector hits and the number of living tracks.}
  \label{fig:baseline}
\end{figure}

A roofline analysis (Fig.~\ref{fig:roofline}) shows that the interaction kernel
is not limited by raw memory bandwidth or floating point performance. XXX @tme
please add a couple of sentences.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{fig/roofline}
  \caption{Roofline plot showing the interaction kernel performance
    plotted against the theoretical memory bandwidth and arithmetic performance
    limitations. The 1/2/4 points are the number of particle tracks processed by
    a single GPU thread.}
  \label{fig:roofline}
\end{figure}

\clearpage

Although this mini-app is far simpler than one that supports the full range of
physics needed for EM shower simulation, it may be instructive to see how
standard recommendations for performance enhancement affect the total runtime.

\begin{itemize}
  \item \emph{Removing the ``grid striding'' wherein a single GPU thread can
    transport multiple tracks sequentially.} Preliminary studies showed
    decreased performance by transporting multiple tracks per thread, so in all
    the results shown here only one track is used per thread. Removing the grid
    striding should reduce the register pressure for the kernel by eliminating
    an unused runtime variable.
  \item \emph{Copying track states into local variables.} Operating on the
    position, direction, and time locally (rather than as pointers to global
    memory) should remove potential aliasing issues and improve potential
    compiler optimizations.
  \item \emph{Copying the random number generator state into a thread-local
    variable.}
    Like with the other track states, the RNG state (XORWOW) is accessed
    directly through global memory. The CURAND documentation notes
    that to increase performance, the generator state can be operated on locally
    but stored in global memory between kernel launches. If the increased
    register usage spills into local memory, then at least the memory usage will
    be coalesced.
  \item \emph{Using a struct-of-arrays rather than array-of-structs for particle
    data.} For the sake of expediency, the \texttt{ParticleTrackState} data is a
    struct with a particle type ID (\texttt{unsigned int}) and an energy
    (\texttt{double}). Conventional CUDA kernels obtain higher memory
    bandwidth when data accesses are ``coalesced,'' which will be more likely
    when each component is a contiguous array. We should note that the
    \texttt{ParticleTrackView} abstraction completely hides this implementation
    change from the tracking kernel and the physics code.
  \item \emph{Preallocating one secondary per interaction.} Rather than using
    the dynamic \texttt{SecondaryAllocator} and its atomic add, preallocate a
    single \texttt{Secondary} as part of the \texttt{Interaction} result.
    Physics kernels that allocate more than one secondary per interaction will
    still need the dynamic allocation, but simpler kernels will no longer need
    the atomic, at the cost of slightly increased memory pressure.
  \item \emph{Splitting the single step kernel into two kernels, one for movement
    and one for interaction.} Smaller kernels tend to have lower register usage
    and therefore higher occupancy.
  \item \emph{Using a 32-bit instead of 64-bit integer for the stack allocator.}
    Smaller data reduces memory bandwidth, and CUDA operations tend to be
    inherently faster for 32-bit types such as the native unsigned int and
    single-precision floats.
\end{itemize}
Figure~\ref{fig:speedup} shows the performance of each separate change in the
list above, as well as a combination of all changes.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{fig/speedups}
  \caption{Incremental (thin colored lines) and cumulative (thick gray line)
  speed up for changes to the mini-app and data structures.}
  \label{fig:speedup}
\end{figure}

Aggregating the speedup values for cases with more than $10^6$ tracks (mean and
$1\sigma$ given for $\mbox{speedup}=\mbox{original}/\mbox{adjusted} - 1$, in
percent):
\begin{itemize}
  \item \emph{Removing the ``grid striding''} had a slightly negative impact
    ($-1.3\% \pm 0.3\%$). Diagnostics showed
    that removing grid striding did in fact increase occupancy from 37.5\% to
    50\%, but this did not translate to improved performance.
  \item \emph{Copying track states into local variables} had a small positive
    effect ($+3.1\% \pm 0.6\%$). Analysis of the PTX assembly code showed that
    with the cost of a few extra arithmetic operations, the improved kernel
    decreased the number of global memory loads from 41 to 29. For this case, a
    25\% reduction in global memory accesses resulted only in a 3\% speedup.
  \item \emph{Copying the RNG states into a local variable} had essentially no effect
    ($-0.8\% \pm 0.5\%$). Examining the emitted PTX code shows almost no change
    between the original and modified version. This suggests that the extensive
    use of inline functions in Celeritas allows the compiler to determine that
    the RNG state (a struct of a type not used anywhere else in the code) cannot
    be aliased or modified by external functions calls, and thus does not have
    to be reloaded between subsequent calls to the RNG functions.
  \item \emph{Using a struct-of-arrays rather than array-of-structs for particle
    data} had zero significant effect. Coalescing memory access in this case
    appears to be unimportant.
  \item \emph{Preallocating one secondary per interaction} improved performance
    more than any other change thus far ($+13.4\% \pm 0.3\%$). There was one
    fewer atomic operation (the detector ``hits'' still remained) and a decrease
    in global
    memory accesses from loading the allocated secondary to process.
  \item \emph{Splitting the single step kernel into two kernels} had the most
    negative effect ($-7.7\% \pm 0.9\%$). This is not unexpected because each
    kernel launch has additional overhead, and each independent kernel has to
    reload data from global memory that might otherwise be stored in registered.
    Still, a 10\% drop in performance might provide a substantial gain in code
    flexibility and extensibility.
  \item \emph{Using a 32-bit instead of 64-bit allocation size} had the most
    positive individual change ($+28.0\% \pm 0.1\%$). In conjunction with the
    secondary preallocation result, this suggests that the atomic operations may
    be the single most expensive aspect of this simple demonstration kernel.
\end{itemize}

The overall speedup of $+37.7\%$ suggests that the faster and fewer atomic
operations negate the performance drop of the atomic-heavy interaction split
kernel.

Loosely stated, occupancy measures the ratio of threads that can be
\emph{active} to the maximum number of threads on a \ac{sm},
which is the core hardware computational component of a CUDA card. Higher
occupancy can hide latency to improve overall kernel performance.
To explore the performance implications of higher occupancy, we recompiled the
``set of combined optimization'' changes with the \verb|--maxrregcount N| NVCC
compiler option to constrain kernel register usage,
which is the limiting factor for the CUDA \ac{sm} occupancy for the demo
interactor kernels. Figure~\ref{fig:occupancy} demonstrates that a higher
kernel occupancy does not always translate to improved performance.

\begin{figure}[htb]
  \centering
  \includegraphics[width=3.5in]{fig/occupancy}
  \caption{Performance ramifications of forcing the register size to be smaller
  for higher occupancy. The blue line is total solve time, the dark red line is
the register usage, and the light red line is local memory usage including
memory spills.}
  \label{fig:occupancy}
\end{figure}

%%---------------------------------------------------------------------------%%
\section{Acknowledgements}

Work for this paper was supported by Oak Ridge National Laboratory (ORNL), which is managed and operated by UT-Battelle, LLC, for the U.S. Department of Energy (DOE) under Contract No. DEAC05-00OR22725.
%%
This research was supported by the Exascale Computing
Project (ECP), project number 17-SC-20-SC. The ECP is a collaborative effort of
two DOE organizations, the Office of Science and the National Nuclear Security
Administration, that are responsible for the planning and preparation of a
capable exascale ecosystem---including software, applications, hardware,
advanced system engineering, and early testbed platforms---to support the
nation's exascale computing imperative.

%%---------------------------------------------------------------------------%%
\bibliography{references}
%%---------------------------------------------------------------------------%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of conference-papers/chep-2021/main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage[shortcuts,acronym,nonumberlist,nogroupskip,nopostdot]{glossaries}
\usepackage[detect-none, binary-units]{siunitx}
\usepackage{color}
\usepackage{c++}
\usepackage{multirow}
\usepackage{authblk}
\usepackage[margin=1in]{geometry}

\usepackage[
  sorting=none,
  citestyle=numeric-comp,
  bibstyle=ieee,
  maxnames=3,
  backend=biber,
  autolang=other,
  bibencoding=auto]{biblatex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Setup

\author[1]{T.M.~Evans}
\author[1]{S.R.~Johnson}
\author[2]{V.D.~Elvira}
\author[3]{P.~Calafiura}

\author[2]{Ph.~Canal}
\author[2]{K.L.~Genser}
\author[2]{S.Y.~Jun}
\author[2]{G.~Lima}
\author[4]{A.~Lund}
\author[3]{V.R.~Pascuzzi}
\author[1]{S.C.~Tognini}

\affil[1]{Oak Ridge National Laboratory}
\affil[2]{Fermi National Accelerator Laboratory}
\affil[3]{Lawrence Berkeley National Laboratory}
\affil[4]{Argonne National Laboratory}

\title{Celeritas - a nascent GPU detector simulation code}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Glossary setup
\setacronymstyle{long-short}
\makeglossaries

\newacronym{mc}{MC}{Monte Carlo}
\newacronym{lhc}{LHC}{Large Hadron Collider}
\newacronym{hpc}{HPC}{high performance computing}
\newacronym{hep}{HEP}{high energy physics}
\newacronym{olcf}{OLCF}{Oak Ridge Leadership Computing Facility}
\newacronym{ecp}{ECP}{Exascale Computing Project}
\newacronym{ornl}{ORNL}{Oak Ridge National Laboratory}
\newacronym{em}{EM}{electromagnetic}
\newacronym{fom}{FOM}{Figure of Merit}

% Hyperref setup
\definecolor{CiteColor}{rgb}{0, 0, 0.55}
\definecolor{LinkColor}{rgb}{0.2, 0.2, 0.2}
\definecolor{URLColor}{rgb}{0.62745098, 0.1254902 , 0.94117647}
\hypersetup{
  linkcolor=LinkColor,
  citecolor=CiteColor,
  urlcolor=URLColor,
  colorlinks=true
}

% SI units
\sisetup{range-phrase = \text{--},
  group-separator={,},
  per-mode=symbol,
  group-minimum-digits=3,
  range-units=single}

% Bib files
\addbibresource{references.bib}
\DeclareFieldFormat*{citetitle}{\em #1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

%%%%%%%%%%%%%%%%%%%%
\section*{Objective}

Upgrades to the \ac{lhc} and its detectors (including CMS, ALICE, and ATLAS)
demand a commensurate increase in modeling and simulation capacity that is out
of reach of traditional CPU-based software. Our objective is to develop a new
detector modeling code, \emph{Celeritas}, with the objective of performing fast
and accurate \ac{mc} particle transport simulations on GPUs and similar advanced
accelerator hardware at the scale needed to satisfy \ac{lhc} for detector
response simulation in particle physics experiments. We are currently developing
an application that \textsl{(i)} is highly optimized for the advanced
architectures that will form the backbone of \ac{hpc} over the next decade and
\textsl{(ii)} will eventually satisfy the detector response requirements as
defined in
\citetitle{the_hep_software_foundation_roadmap_2019}
\cite{the_hep_software_foundation_roadmap_2019}. Celeritas is leveraging
architectural techniques and algorithms from the \ac{ecp} project \emph{ExaSMR},
a GPU-accelerated neutral particle \ac{mc} application \cite{ecp2019}, and
models from the Geant4 detector simulation toolkit \cite{geant4}. We note that
Geant4 is a fully general \emph{toolkit} capable of addressing many modeling and
simulation transport problems, whereas Celeritas is a highly specialized
\emph{application} focused on the most computationally intensive calculation
currently performed in experimental detector simulations: time- and
energy-dependent detector response.  Celeritas is planned to complement Geant4
by providing a pathway for the acceleration of a subset of charged particle
transport modeling use cases on HPC architectures. The long term goal is to
re-incorporate these methodologies back into the Geant framework for broader
application use.

% Celeritas is planned to complement Geant4 that both leveraging and feeding back into Geant4 physics algorithm while extending Geant4 by providing acceleration of a subset of use cases for HPC environments.

%(We need to make clear that Celeritas is not a unique product in the sense that it will utilize Geant components and then evolve independently diverging and competing with Geant4. What I understood is that Celeritas would be built on top of each Geant4 release, adapting each improved version of G4 (including updated physics) to run on GPUs. Even if we still do not know the adaptation/validation/support model, we should not give the impression that Celeritas is an attempt to replace Geant4.)%

%%%%%%%%%%%%%%%%%%%%%
\section*{Background}

The current state-of-the-art \ac{mc} application for simulating the passage of
particles through matter is Geant4, a toolkit that encompasses the simulation of
all interactions described by the Standard Model of Particle Physics:
\ac{em}, weak, and strong interactions. A drawback to Geant4 for
\ac{lhc} modeling is the very long run times required to perform simulations.
One pathway to address this limitation is to utilize advanced accelerator (GPU)
architectures that are now available at DOE leadership computing facilities on
systems such as Summit at the \ac{olcf}. These architectures are becoming
increasingly available on institutional-scale computing clusters as well.
However, porting and optimizing \ac{mc} algorithms to work efficiently on GPU
architectures is a difficult task, particularly in CPU-only codes such as Geant4
that rely on extensive runtime polymorphism, random walks and branching. GPU
coding environments are highly sensitive to memory access patterns, device
occupancy, and thread divergence. Furthermore, common \C++ language idioms that
are heavily used in Geant4 transport and physics routines, including inheritance
and dynamic memory allocation, cannot be used in an efficient manner in device
code.

Recent work in the \emph{ExaSMR: Coupled Monte Carlo Neutronics and Fluid Flow
Simulation of Small Modular Reactors} project within the DOE \ac{ecp}
\cite{ecp2019} has demonstrated significant performance for neutron MC transport
on GPU architectures \cite{hamilton_continuous-energy_2019}. These architectural
advancements are implemented in the \ac{ornl} \ac{mc} application Shift
\cite{pandya_implementation_2016}.

Celeritas is leveraging the GPU-optimized transport techniques demonstrated in
Shift and the physics models implemented in Geant4 to provide a charged-particle
transport application that achieves optimal performance on current and future
leadership-class computing architectures. The objective is to create an
application that addresses the most compute-intensive component of the HEP
detector simulation workflow: the simulation of the \ac{em} processes in the
\ac{em} calorimeter. In this sense, Celeritas will be an application that
complements standard Geant4 by targeting only one of the the computational
bottlenecks of large detector simulations.

%%%%%%%%%%%%%%%%%%%
\section*{Approach}

The transport solver within this new application must be performant on
accelerator architectures for the detector models and physics necessary to model
CMS, ATLAS and other current and future \ac{lhc} and particle physics
experiments. Ultimately this requires the ability to transport the complete
standard model of particle types including \ac{em}, strong, and weak
interaction physics. In \textcite{hamilton_continuous-energy_2019} we developed
a GPU-performant \ac{mc} transport algorithm for neutrons that achieves greater
than $60\times$ speed up per-node on Summit versus running on CPU cores (through
MPI) alone. However, there are several distinctions between that work and the
necessary capabilities required for particle physics detector modeling. The
reactor and nuclear technology applications that algorithm targets are not
characterized by large showers of secondary particles, and because the particles
are neutral, there are no \ac{em} field interactions.

The current work focuses on the development of a transport code for
\ac{em} interactions of photons and charged leptons. This particular
choice as a first step towards full Standard Model \ac{mc} transport is due to
simplicity and usefulness. Strong interactions present high-stakes challenges to
implement, and weak interactions does not just encompass a vast list of
particles with different branching ratios but also entail following the produced
hadrons that depend on strong interaction processes in order to achieve a direct
comparison with Geant4. Conversely, \ac{em} interactions can be limited
to ionization energy loss, bremsstrahlung, pair production, Cherenkov radiation,
and photonuclear interactions. As for usefulness, a proof-of-concept should
provide a clear form of comparison between Celeritas and Geant4. In this
scenario, leptons such as electrons and muons constitute perfect probes, as
these particles are widely used for detector calibration purposes
\cite{atlas_calibration_e,atlas_calibration_mu}.  Finally, \ac{em}
interactions processing, due to the large cardinality of particles, takes most
of the run-time of a simulation, thus accelerating them provides the biggest
benefits.

In \ac{lhc} detector response modeling applications, the desired outputs are
time- and particle-dependent energy depositions in user-identified cells
(sensitive detectors) correlated to each generating event (e.g.~proton-proton
collision). The basic neutral particle GPU transport algorithm developed within
ExaSMR must be extended to treat continuous processes such as multiple
scattering, generation of massive secondary particle showers, as well as
tracking in the (electro-)magnetic field. To ensure that progress can be made
towards a fully featured application that meets these transport requirements, we
must address these potential bottlenecks in a systematic way early in the GPU
transport algorithm development.

As part of the GeantV and \ac{ecp} Geant exascale pilot projects, the navigation
(geometry tracking) capabilities of Geant4 have been extracted into a
stand-alone library called \emph{VecGeom} \cite{apostolakis_towards_2015}. Since
VecGeom is written to support both \C++/CPU and CUDA/GPU targets and is able to
read \texttt{GDML} geometry definitions, it provides a production geometry
capability for transport on GPUs.

%%%%%%%%%%%%%%%%%%
\section*{Outlook}

(An outlook section is important in the context of a Snowmass LOI or white
paper, where we summarize the vision in the context of the previously described
challenges and proposed solutions.)

% References
\pagebreak
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
